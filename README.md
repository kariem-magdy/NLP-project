# Arabic Text Diacritization System

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c)
![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-yellow)
![License](https://img.shields.io/badge/License-MIT-green)

A comprehensive Natural Language Processing (NLP) pipeline for automatic Arabic diacritization. This project implements two state-of-the-art approaches: a feature-rich **BiLSTM-CRF** baseline and a fine-tuned **Transformer (AraBERT)** model. It is designed for high performance, modularity, and ease of deployment.

---

## ğŸš€ Key Features

### 1. Advanced BiLSTM-CRF Architecture
A robust sequence labeling model incorporating multiple linguistic features:
- **Character Embeddings**
- **Word Embeddings**
- **FastText Embeddings** (optional)
- **Bag-of-Words (BoW)** sentence features
- **TF-IDF** sentence features
- **CRF Layer** for modeling diacritic transitions

### 2. Transformer Fine-Tuning
- Fine-tunes **AraBERT** or similar models for token classification  
- Automatic subword alignment  
- Uses HuggingFace `Trainer` API  

### 3. Production Ready
- **Flask API** for real-time web demo  
- **Config-driven design**  
- **Clean inference pipeline**

---

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ build_vocab.py           # Build vocabularies & feature vectorizers
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ setup.sh                 # Optional setup script
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.txt
â”‚   â”œâ”€â”€ val.txt
â”‚   â””â”€â”€ fasttext_wiki.ar.vec # Optional FastText vectors
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/              # Model checkpoints
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ processed/           # vocab + feature vectorizers
â””â”€â”€ src/
    â”œâ”€â”€ app/                 # Flask application
    â”œâ”€â”€ config.py            # Hyperparameters & flags
    â”œâ”€â”€ features.py          # BoW, TF-IDF, FastText feature logic
    â”œâ”€â”€ preprocess.py        # Normalization & label extraction
    â”œâ”€â”€ data/                # Dataset + collate function
    â”œâ”€â”€ models/              # BiLSTM-CRF & Transformer models
    â”œâ”€â”€ train/               # Training scripts
    â”œâ”€â”€ infer/               # Inference scripts
    â””â”€â”€ eval/                # DER evaluation
```

---

## ğŸ› ï¸ Installation

### 1. Prerequisites
- Python 3.8+
- (Optional but recommended) CUDA-compatible GPU

### 2. Setup

```bash
git clone https://github.com/kariem-magdy/NLP-project.git
cd "NLP project"
pip install -r requirements.txt
```

---

## ğŸ“Š Data Preparation

Before training, generate vocabularies and feature vectorizers.

1. Ensure `train.txt` and `val.txt` are inside the **data/** folder  
2. (Optional) Download FastText vectors and rename to:

```
data/fasttext_wiki.ar.vec
```

3. Run:

```bash
python build_vocab.py
```

Artifacts will be saved to:  
`outputs/processed/`

---

## ğŸ§  Usage

### ğŸ”¹ Train BiLSTM-CRF Model

```bash
python -m src.train.train_bilstm
```

Output:
- `outputs/models/best_bilstm.pt`
- DER logged to console/logs

---

### ğŸ”¹ Train Transformer (AraBERT)

```bash
python -m src.train.train_transformer
```

---

### ğŸ”¹ Inference (CLI)

```bash
python -m src.infer.infer
```

Modify the test sentence inside `src/infer/infer.py`.

---

### ğŸ”¹ Web Demo (Flask)

```bash
python -m src.app.app
```

Visit:

```
http://localhost:5000
```

---

## ğŸ“‰ Evaluation Metric

The system uses **Diacritic Error Rate (DER)**:

```
DER = (Incorrect Predictions / Total Valid Characters) Ã— 100%
```

---

## âš™ï¸ Configuration (`src/config.py`)

```python
# Feature Flags
use_word_emb = True
use_fasttext = False
use_bow = True
use_tfidf = True

# Hyperparameters
char_emb_dim = 128
lstm_hidden = 256
batch_size = 32
epochs = 20
lr = 1e-3
```

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
